<!--- Code generated by preguide from guide/en.markdown; DO NOT EDIT. --->


### Introduction

At [FOSDEM 2022](https://fosdem.org/2022/), [Marcel van
Lohuizen](https://twitter.com/mpvl_) and [Paul
Jolly](https://twitter.com/_myitcv) presented ["A practical guide to CUE:
patterns for everyday
use"](https://fosdem.org/2022/schedule/event/cue_pratical_guide/). A recording
of the talk will soon be made available on https://video.fosdem.org/2022.
Meanwhile, the slides are available
[here](https://docs.google.com/presentation/d/1BycB_WevWQfzSoyGHuAIQOWQhbq4npU2aPVs14tiDa4/edit?usp=sharing).

The main goal of the talk is to present a practical guide to CUE with patterns
and techniques to help drive CUE adoption in your project or company.

The talk features a demonstration in which we imagine what such an adoption path
might look like for Acme.com, a fictional company who have a simple Go,
Kubernetes and grpc-based setup. The demo shows how and where the company
gradually adopt CUE.

This guide allows you to recreate the demo locally, step-by-step, using a Docker
container for convenience. We explain how to get started below, but first some
background on Acme.com.

### Acme.com

Acme.com's entire system (it's very small) is composed of two fictional
services, each a Go program:

* `quoteserver`
* `funquoter`

`funquoter` is incredibly simple and for now does not expose any API.
Every 10 seconds, as a test, `funquoter` requests a number of quotes from
`quoteserver` and logs the famous and recognisable proverbs to stdout.

The `quote` package imported by both `quoteserver` and `funquoter`
defines a grpc-based `Quoter` service with a single method `Quote`.

The infra team are responsible for the upkeep of the system.

For the sake of keeping the example simple, Acme.com keep all their code a
single or mono repository (just to be clear this is absolutely not a requirement
for the use of CUE).

### Running the demo locally

To play around with CUE and get familiar with what is possible, you can recreate
the demo yourself locally from start to finish.

The simplest way to get started is to use the `cueexamples/fosdem2022` Docker
image:

```
$ git clone https://github.com/cue-examples/fosdem2022
$ cd fosdem2022
$ ./docker/run.sh
```

The last command will drop you into a minimal Docker container that has all the
tools necessary for working through the example from start to end, following the
steps outlined below.

The `workdir` directory inside the `fosdem2022`
directory you cloned is mounted at `/workdir` inside the Docker
container. You can therefore edit files using your favourite editor outside of
the container, and simply issue commands within the container.

You are now ready to get started! Run each of the commands listed below within
the Docker container, and create/update files according to the path shown as a
comment at the top of each file using your editor outside of the container.

If you encounter any issues working through the demo, please [raise an
issue](https://github.com/cue-examples/fosdem2022/issues/new).

Good luck!

### Requirements

This demo uses CUE version:

<pre><code>$ cue version
cue version v0.4.2 linux/arm64
</code></pre>

and Go version:

<pre><code>$ go version
go version go1.17.6 linux/arm64
</code></pre>

Type or copy/paste these commands into your container, and you should see the
same output.

### Preiminary setup

Running within the container, change to the working directory where the `workdir` directory is mounted.

<pre><code>$ cd /workdir
</code></pre>

This corresponds to `workdir` in the clone of the demo.

Create a `k3s` registry and cluster:

<pre><code>$ k3d registry create --no-help registry.acme.com --port 5000
$ k3d cluster create acme.com --registry-use k3d-registry.acme.com:5000
kubectl cluster-info
</code></pre>

Tell `kubectl` which cluster to use:

<pre><code>$ export KUBECONFIG=&#34;$(k3d kubeconfig write acme.com)&#34;
</code></pre>

Build images for `funquoter` and `quoteserver` services:

<pre><code>$ docker buildx build --quiet --push -t localhost:5000/fosdem2022/funquoter -f funquoter/Dockerfile .
sha256:bba8c540db5466f7a2bc1b6231cf23a59ea9cb89caf979f9aec3e56f93fafc21
$ docker buildx build --quiet --push -t localhost:5000/fosdem2022/quoteserver -f quoteserver/Dockerfile .
sha256:c7b6b5f2a433f5f869a8457bd547b7fa3e79442bb0b3049c6715dec8ad65608f
</code></pre>

Start the `funquoter` and `quoteserver` services:

<pre><code>$ kubectl apply -f quoteserver/kube.yaml
service/quoteserver created
deployment.apps/quoteserver created
$ kubectl apply -f funquoter/kube.yaml
service/funquoter created
deployment.apps/funquoter created
</code></pre>

### Exploring the example

You should still be at the root of the example:

<pre><code>$ pwd
/workdir
</code></pre>

Acme.com have defined all of their services under a single Go module:

<pre><code>$ go list -m
acme.com/x
$ go list ./...
acme.com/x/funquoter
acme.com/x/quote
acme.com/x/quoteserver
</code></pre>

The `quoteserver` and `funquoter` packages also contain the k8s
declarations and `Dockerfile` for each service:

<pre><code>$ ls funquoter quoteserver
funquoter:
Dockerfile  kube.yaml  main.go

quoteserver:
Dockerfile  kube.yaml  main.go
</code></pre>

Confirm your system is now running:

<pre><code>$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
quoteserver-f86b5b747-424lz   1/1     Running   0          32s
funquoter-7c64784df6-lgzbt    1/1     Running   0          32s
</code></pre>

To see `funquoter` in action you can watch its logs:

<pre><code>$ kubectl logs -l app=funquoter
in funquoter
quotes: [&#34;Concurrency is not parallelism.&#34;]
</code></pre>

For now `funquoter` is only requesting a single quote from `quoteserver`, and
`quoteserver` is hard coded to return the well-known [Go
proverb](https://go-proverbs.github.io/) shown above.

### Validation

Now the `funquoter` team have been keeping up with the latest trends and have
heard about CUE They are keen to try and adopt CUE in their workflow .  But a
strict requirement for now is they want to continue maintaining YAML files for
Kubernetes service declarations So where to start?

A good place start is to use CUE to validate existing YAML K8s definitions and
to do so incrementally. CUE's compositional model will allow the team to start
with basic constraints, adding more validation bit by bit.

The `funquoter` team start by declaring some basic constraints that
describe the structure of their service.

They do so in `/workdir/funquoter/schema.cue`.

Change to the `funquoter` directory:

<pre><code>$ cd funquoter
</code></pre>

Create `schema.cue` using your favourite editor within the `workdir/funquoter`
directory in the `fosdem2022` directory you cloned from GitHub:

```cue
// /workdir/funquoter/schema.cue

package kube

// First part of demo: using a file directly to verify a stream of YAML objects.
//
//    $ cue vet kube.cue -d Object kube.yaml
//
// to verify a specific type:
//
//    $ cue vet kube.cue -d Service kube.yaml

// This explains how we tell Kubernetes objects apart.
Service: kind:    "Service"
Deployment: kind: "Deployment"

Object: Deployment | Service

// Label and selector policy. These constraints fulfil two purposes:
//
// 1) automatically derive standard labels as a convenience to the user,
// 2) check that the value of a namesake preexisitng label is correct.
//
// In general, constraints in CUE often serve the purpose of both validation and
// generation.

Service: {
	// Note: no need to declare metadata.name here, even though it is referenced
	// below. Types are typically enforced by mixing in complete schema
	// definitions that are defined seperately from policy.
	metadata: labels: app: metadata.name
	spec: selector: app:   metadata.name
}
Deployment: X={
	// X is a value alias for deployment that allows us to refer
	// to metadata without needing to declare it.
	spec: template: metadata: labels: app: X.metadata.name
}

// Require monitoring. Note that the top-level schema only suggests
// monitoring is enabled as a default. This definition goes a bit further
// in making it a strict requirement.
//
// Style tip: put "boilerplate path" on first line and start new lines with
// path elements that are more meaningfull to the aspect. In this case, the
// important part is that we are adding an annotation. The fact that these
// live in the metadata section is less relevant for the understanding.
Deployment: spec: template: metadata:
	annotations: "prometheus.io/scrape": "true"

// Enforce the use of the acme.com container registry
Deployment: spec: template: spec:
	containers: [...{
		image: =~"""
			^k3d-registry.acme.com:5000/
			"""
	}]
```

The team declare an `Object` to be a `Deployment` or `Service` using a
[disjunction](https://cuelang.org/docs/references/spec/#disjunction).  For those
familiar with Kubernetes declarations, `Deployment` and `Service` objects are
identifiable by the `kind` field We refer to `kind` field acts as a
discriminator - it determines whether an object is a deployment or a service.

For those familiar with CUE, note that the team are not writing
[definitions](https://cuelang.org/docs/references/spec/#definitions-and-hidden-fields).
They don’t yet have a schema for all the fields - they only have an incomplete
declaration.

Similarly, they could have written one big declaration for `Deployment` and
`Service`, but presented this way you can see how it is easy to group related
rules together.

The team enforce the use of the Acme.com image registry for all their deployments, by constraining
the `image` field to match the regular expression shown.

For any service declaration, the team constrain the `metadata` name to be
consistent with the `app` `label`, and the `spec` `selector` `app`.  For
deployments, the team use a value alias, `X`, to access the top-level `metadata`
field without having to define it. They then constrain the `metadata` `name` to
be consistent with the `spec` `template` `metadata` `label` `app`.

CUE's compositional model allows the team to break down their configuration into
important pieces piecemeal, gradually refining their definitions to be more
specific

Note all of this work is entirely independent of the quoteserver and other teams
Not only that, the workflow of editing YAML files remains untouched.

The team use `cue vet` to validate their Kubernetes configuration using
the constraints in `schema.cue`.

<pre><code>$ cue vet schema.cue -d Object kube.yaml
</code></pre>

The `-d` flag specifies an expression that selects the schema to apply to
non-CUE files.  In this case you want to vet `kube.yaml` against `Object`.  `cue
vet` processes the stream of YAML. The `Object` schema matches YAML objects by
the `kind` field, and applies the relevant constraints.

Thankfully the team's configuration passes! If it didn't you would see some
output from the command above. Indeed try and change `funquoter/kube.yaml` and
re-run the `cue vet` command to see how CUE alerts you when the data does not
satisfy the schema.

At this stage you have only validated the Kubernetes configuration satisfies the
basic schema declared in `schema.cue`. There should be no changes to apply to
the `funquoter` service, something you can confirm via:

<pre><code>$ kubectl apply -f kube.yaml
service/funquoter unchanged
deployment.apps/funquoter unchanged
</code></pre>

### File organization: sharing CUE validation and policy

The Acme.com infrastructure team provides monitoring services to other teams.
They want to declare some CUE constraints on deployment configurations.

The infrastructure team create a directory structure of their own:

<pre><code>$ cd /workdir
$ mkdir -p /workdir/infra/mon
</code></pre>

and specify the following constraints:

```cue
// /workdir/infra/mon/mon.cue

package mon

// This aspect of Deployment defaults Prometheus scraping to true.
// As a policy, teams are expected to set up Prometheus scraping. They can
// disable it explicitly if needed.
Deployment: spec: template: metadata: annotations:
	"prometheus.io/scrape": *"true" | "false"

// ACME Co's standardized app framework has a built in HTTP handler for health
// checks. This sets this up by default. As a policy, if teams opt to use
// a different framework, then still need to implement this handler.
//
// NOTE: in a real-life scenario, this would probably not be maintained by
// the monitoring team. :)
Deployment: spec: template: spec: containers: [...{
	livenessProbe: {
		httpGet: {
			path: "/debug/health"
			port: *8080 | int
		}
		initialDelaySeconds: *40 | >10
		periodSeconds:       *3 | int
	}
}]
```

They specify that a deployment should have Prometheus scraping on by default.
Teams will have to explicitly disable it to turn it off.

The second aspect specifies that a deployment _must_ have a liveness probe at a
predetermined path.  This can be handy, for instance, if an organisation uses a
common application framework that standardizes on these things.

There are a few things to note here. There are two entries for `Deployment`.
This is realy composition in action: CUE will automatically combine these
together.  To reemphasize, in general, we think it is good style to split such
“rules” into aspects of related functionality, rather than having one big blob
per type. This enhances readability and makes it easier to refactor code.

Another style point here is that paths are split to put the boilerplate path
on the first line, and then the more meaningful part of the path on the second
line.  So on the first line you can immediately see the type of object it applies
to, whereas on the second line you can quickly scan to see the essence of what
the rule is doing.

To use these constraints via [the "pull"
model](https://docs.google.com/presentation/d/1BycB_WevWQfzSoyGHuAIQOWQhbq4npU2aPVs14tiDa4/edit#slide=id.g10e62fd42dd_1_624),
the `funquoter` team need to be able to import the `infra/mon` package. To do
that, the packages must live within a [CUE
module](https://cuelang.org/docs/concepts/packages/):

<pre><code>$ cue mod init acme.com/x
</code></pre>

Returning to the `funquoter` directory:

<pre><code>$ cd funquoter
</code></pre>

The `funquoter` team now further constrain a `Deployment` by the infrastructure
team's `Deployment` constraints:

```cue
// /workdir/funquoter/mon.cue

package kube

import "acme.com/x/infra/mon"

// mon.Deployment is the policy defined for Deployment as defined by the
// monitoring team.
Deployment: mon.Deployment
```

You can use `cue export` to see the result of evaluating the `kube` package and
`kube.yaml`: The `–out` flag indicates the result should be rendered as YAML:

<pre><code>$ cue export :kube --out yaml -d Object kube.yaml
apiVersion: v1
kind: Service
metadata:
  name: funquoter
  labels:
    app: funquoter
spec:
  selector:
    app: funquoter
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: funquoter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: funquoter
  template:
    metadata:
      labels:
        app: funquoter
      annotations:
        prometheus.io/scrape: &#34;true&#34;
    spec:
      containers:
        - name: application
          image: k3d-registry.acme.com:5000/fosdem2022/funquoter
          imagePullPolicy: Always
          livenessProbe:
            httpGet:
              path: /debug/health
              port: 8080
            initialDelaySeconds: 40
            periodSeconds: 3
          args:
            - -addr=quoteserver:80
            - -requote=10s
</code></pre>

Compare the output above with the contents of `kube.yaml` to see how the
liveness probe configuration has been added.

Note: this doesn’t enforce that it is implemented, of course. That has to be
dealt with at other locations.

But CUE also has another mechanism that conforms more to the cross-cutting
nature of configuration.

For example, suppose the monitoring team wants to enforce the constraints
shown above as matter of policy, rather than relying on each team pulling them
in explicitly through an import. This is referred to as [the "push"
model](https://docs.google.com/presentation/d/1BycB_WevWQfzSoyGHuAIQOWQhbq4npU2aPVs14tiDa4/edit#slide=id.g10e62fd42dd_1_590).

To see that in action, first remove the file in which the `funquoter` team
imported (pulled) the monitoring team's constraints:

<pre><code>$ rm mon.cue
</code></pre>

To use the "push" model, the monitoring team apply the same change as before,
but at higher level directory so that it spans different packages across the
organization.

```cue
// /workdir/schema.cue

package kube

import "acme.com/x/infra/mon"

// Enforce monitoring policies for all teams
Deployment: mon.Deployment
```

This is really the same change as the `funquoter` team made earlier, with the
main difference being that now these constraints are added outside the control
of the teams of the relevant subdirectories, meaning that the same constraints
now function as a policy rather than as a data template.

Verify that the liveness probe is still added to the resulting exported
`funquoter` service configuration:

<pre><code>$ cue export :kube --out yaml -d Object kube.yaml
apiVersion: v1
kind: Service
metadata:
  name: funquoter
  labels:
    app: funquoter
spec:
  selector:
    app: funquoter
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: funquoter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: funquoter
  template:
    metadata:
      labels:
        app: funquoter
      annotations:
        prometheus.io/scrape: &#34;true&#34;
    spec:
      containers:
        - name: application
          image: k3d-registry.acme.com:5000/fosdem2022/funquoter
          imagePullPolicy: Always
          livenessProbe:
            httpGet:
              path: /debug/health
              port: 8080
            initialDelaySeconds: 40
            periodSeconds: 3
          args:
            - -addr=quoteserver:80
            - -requote=10s
</code></pre>

### The tooling layer: formalizing the process

Thus far you have manually verified Kubernetes YAML files with CUE on the
command line.

You can automate that process with the tooling layer: a declarative workflow
mechanism supported by CUE.

Using higher-level commands you will validate the YAML files as a byproduct,
rather than having to do this explicitly.

There are various ways to accomplish this, so what follows is not supposed to be
prescriptive but rather serve to highlight the possibilities.

The goal here is to write a single workflow, capturing common functionality,
that can be used — and possibly tailored — by all teams.

Declare a `kube_tool.cue` file as follows:

```cue
// /workdir/kube_tool.cue

package kube

import (
	"strings"
	"encoding/yaml"
	"encoding/json"

	"tool/file"
	"tool/cli"
	"tool/exec"
)

// -- Global tasks

// glob matches the YAML files in the current directory.
globYAML: file.Glob & {
	glob: "*.yaml"
}

open: {
	for _, f in globYAML.files {
		(f): file.Read & {
			filename: f

			contents: _
			data:     yaml.UnmarshalStream(contents)
		}
	}
}

// allTasks lists all tasks run to fetch the configuration. Other tasks
//
allTasks: {glob: globYAML, open}

// -- Linking results into schema

// Put results into the object map as defined in our top-level schema.
objByKind: {
	for v in open for obj in v.data {
		(strings.ToLower(obj.kind)): (obj.metadata.name): obj
	}
}

// allObjects is a list of all Kubernetes objects declared in CUE and YAML
// files.
allObjects: [ for objs in objByKind for obj in objs {obj}]

// -- Commands

// Ensure that commands run after the global tasks.
// TODO: flow should handle this automatically. It currently does not handle
// the indirection of references through global definitions correctly.
command: [string]: $after: allTasks

// print prints all Kubernetes objects as a stream of marshalled JSON.
command: print: cli.Print & {
	text: json.MarshalStream(allObjects)
}

// apply sends the k8s objects off to kubectl.
command: apply: exec.Run & {
	cmd:   "kubectl apply -f -"
	stdin: yaml.MarshalStream(allObjects)
}
```

And update `schema.cue` in the root of the repository as follows:

```cue
// /workdir/schema.cue

package kube

// This schema can be used anywhere within the module to verify files using
// the following, or similar, commands:
//
//    $ cue vet :kube -d Object kube.yaml
//
// Here, :kube tells CUE to load the kube package scoped from the current
// directory, creating a "package instance". A package instance includes all
// constraints defined in the package (in this case "kube") from the current
// directory and all its ancestor directories within the same CUE module, which
// is marked by the cue.mod directory.

import (
	"acme.com/x/infra/mon"
)

// This explains how we tell Kubernetes objects apart.
//
// For those already familiar with CUE, note that Service and Deployment
// are not defined as definitions (#Service and #Deployment). This is a result
// of how these definitions evolved. They started as constraints on YAML,
// without any underlying schema. In other words, the schema were not known.
// As soon as schema are mixed in (see kube_defs.cue),  however, Service and
// Deployment will behave identically.
//
// A drawback of using "regular" fields for these definitions is that they will
// be output as part of export and friends. As a matter of style, we
// distinguish the types from data fields by starting them with an uppercase
// letter, but export does not similarly distinguish such fields.
//
// In this application that is not an issue that the types are considered to
// be data. In case it is, though, there are several alternatives:
// 1) move the types to a separate packages, allowing to refer to them as,
//    for instance, types.Service and types.Deployment. This looks neat, but
//    makes it harder to use hierarchical constraints.
// 2) use hidden fields: _Service and _Deployment. Like definitions, hidden
//    fields are excluded from exports.
// 3) we could support a @export(ignore) attribute to exclude fields.
//    We would love to hear from you if you are interested in this feature.
Service: kind:    "Service"
Deployment: kind: "Deployment"

Object: Deployment | Service

// We define a standard place for all our Kubernetes objects to live,
// collated by kind.
objByKind: service: [string]:    Service
objByKind: deployment: [string]: Deployment

// Style tip: rather than grouping constraints per Kubernetes object type, we
// group constraints by topic. This makes it easier to see the relationship
// between constraints and also makes it much easier to move constraints around
// later.
//
// A special case of this is defining default values. As a general guideline,
// default values should not be defined within schema, but rather as a seperate
// aspect that then may or may not be mixed in unconditionally by a user.

// Our monitoring specialists have defined these to help ensure that
// our services get monitored properly.
Deployment: mon.Deployment

// Label and selector policy for Kubernetes objects: this section defines a set of
// standardized labels for service. This standard was adopted from our friendly
// folks of the Frontend team.
//
// Style tip: to make it clearer that two declarations belong together one could
// us an embedding to group them. Alternatively, one can have the convention of
// not using an extra newline between definitions. The embedding approach,
// though, has the advantage that comments will be grouped logically in the
// parse tree. This may matter for automation.
{
	Service: {
		metadata: labels: app: metadata.name
		spec: selector: app:   metadata.name
	}

	Deployment: X={
		// We can refer to a section (here metadata) without having it declared
		// by adding an alias to the enclosing value. As a general rule one
		// should use a value alias (a: X=b) as opposed to a field alias (X=a:
		// b) to the most-inner scope of the section one wishes to refer to.
		spec: template: metadata: labels: app: X.metadata.name
	}
}

// This Service aspect defines a set of preferred values for ports.
// Teams can omit standard values in their configuration, causing any
// non-default values to stand out.
Service: spec: ports: [...{
	port:       *8080 | int
	targetPort: *port | int
	protocol:   *"TCP" | "UDP"
}]
```

Refer to the
[slides](https://docs.google.com/presentation/d/1BycB_WevWQfzSoyGHuAIQOWQhbq4npU2aPVs14tiDa4/edit#slide=id.g10e62fd42dd_1_453)
to see how the description that follows corresponds to the file you just
created.

The first step for control flow is to load the YAML files. We plan to have
native support for having JSON and YAML included in packages directly, but for
now you have to do this.

A CUE workflow consists of task. Tasks are CUE values that tell CUE what action
to take, describing inputs and outputs.  All tasks are defined in the
[`pkg/tool/...`](https://pkg.go.dev/cuelang.org/go/pkg/tool) package hierarchy.

A task gets triggered when all its input fields are specified, writing the
results to the output fields.

CUE automatically determines dependencies between tasks based on references
between them. So if an input one task references the output of another task, it
is automatically run after it.

In the file above, you can see two different tasks: the first task, `globYAML`, finds
all YAML files in the current directory. The
[comprehension](https://cuelang.org/docs/references/spec/#comprehensions) then
creates a `file.Read` task for each YAML file, each identifiable by a different
path, in this case `open: \(f)`, where `f` is a reference to the filename.

The tasks you see here are global tasks, meaning that they are not associated
with a specific command.

For more details on the `tool/file.Read` and `tool/file.Glob` tasks, see [the
`tool/file`
documentation](https://pkg.go.dev/cuelang.org/go@v0.4.1/pkg/tool/file).  Tasks
are a bit like functions. But instead of it being called, they are structs with
input and output fields that get set upon execution.

After the tasks are run, you need to do something with the read data.

Rather than reading directly from the tasks, you will push the read result into a
map called `objByKind`, which sorts the objects by kind and name.

Of course there are many more object types, but here we focus on `Service` and
`Deployment`.

Then the next step is to actually pull the task results and put them in this
map. That is done by means of a comprehension that populates `objByKind`. We
adopt the convention that all data fields are `camelCase`, so we convert the
kind names by lowercasing them. (`strings.ToCamel` is an example of referencing
non-CUE code.)

As most commands that you will define need a list of all these objects, create
one ahead of time via `allObjects`.

Users can invoke workflows by means of commands, which are basically a grouping
of tasks that can be run by the cue command.

The two commands used in the demo are `print` and `apply`. Each command has only
one task itself. But they can have many tasks in reality. In fact, these
commands actually consist of many tasks, because as you can see, they reference
`allObjects`, which in turn references the global tasks described above.  CUE
runs all dependent tasks as part of the workflow.

Running `cue cmd print` gives you a JSON stream of all objects:

<pre><code>$ cue cmd print
&#123;&#34;apiVersion&#34;:&#34;v1&#34;,&#34;kind&#34;:&#34;Service&#34;,&#34;metadata&#34;:&#123;&#34;name&#34;:&#34;funquoter&#34;,&#34;labels&#34;:&#123;&#34;app&#34;:&#34;funquoter&#34;&#125;&#125;,&#34;spec&#34;:&#123;&#34;selector&#34;:&#123;&#34;app&#34;:&#34;funquoter&#34;&#125;,&#34;ports&#34;:[&#123;&#34;protocol&#34;:&#34;TCP&#34;,&#34;port&#34;:80,&#34;targetPort&#34;:3000&#125;]&#125;&#125;
&#123;&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;Deployment&#34;,&#34;metadata&#34;:&#123;&#34;name&#34;:&#34;funquoter&#34;&#125;,&#34;spec&#34;:&#123;&#34;replicas&#34;:1,&#34;selector&#34;:&#123;&#34;matchLabels&#34;:&#123;&#34;app&#34;:&#34;funquoter&#34;&#125;&#125;,&#34;template&#34;:&#123;&#34;metadata&#34;:&#123;&#34;labels&#34;:&#123;&#34;app&#34;:&#34;funquoter&#34;&#125;,&#34;annotations&#34;:&#123;&#34;prometheus.io/scrape&#34;:&#34;true&#34;&#125;&#125;,&#34;spec&#34;:&#123;&#34;containers&#34;:[&#123;&#34;name&#34;:&#34;application&#34;,&#34;image&#34;:&#34;k3d-registry.acme.com:5000/fosdem2022/funquoter&#34;,&#34;imagePullPolicy&#34;:&#34;Always&#34;,&#34;livenessProbe&#34;:&#123;&#34;httpGet&#34;:&#123;&#34;path&#34;:&#34;/debug/health&#34;,&#34;port&#34;:8080&#125;,&#34;initialDelaySeconds&#34;:40,&#34;periodSeconds&#34;:3&#125;,&#34;args&#34;:[&#34;-addr=quoteserver:80&#34;,&#34;-requote=10s&#34;]&#125;]&#125;&#125;&#125;&#125;

</code></pre>

The `apply` command lets you `kubectl apply` the result of the CUE evaluation,
the combined result of yaml and CUE constraints you saw as the result of `cue
export`:

<pre><code>$ cue cmd apply
service/funquoter unchanged
deployment.apps/funquoter configured
</code></pre>

As expected there is a change to apply here, specifically the liveness probe you
saw earlier which is now mandated as policy by the monitoring team.

As before, running each of these commands will trigger validation errors if
`kube.yaml` does not satisfy the constraints declared by the `funquoter` and
monitoring teams. Experiment by changing the hostname of the `funquoter`
deployment image URL and then running `cue cmd apply` again.


### Importing schema: making is all Kubernetes aware

So far the `funquoter` team have been declaring their own constraints, enforcing
consistency and templating their Kubernetes service But surely the team don't
have to declare the entire service and deployment schemas by hand?  Is there not
a source of truth they can refer to and use that to validate their YAML?

For Kubernetes the source of truth is Go code

CUE natively supports importing and exporting data _and_ schema from multiple
encodings:

Data encodings
JSON
YAML
Schema encodings
OpenAPI
Protobuf
JSON Schema
Language encodings
Go

We have plans to support many many more. Indeed please ensure we have [open
issues](https://github.com/cue-lang/cue/issues) for encodings that you consider
a priority. That helps to make transparent the priority, as others can leave
emoji in support of the proposal

The team want to validate the correctness of the `funquoter` service by mixing
in the Kubernetes types imported from Go code.

They do that by further constraining the `Service` and `Deployment` fields by
the imported definitions (again within the root of the repository):

```cue
// /workdir/kube_defs.cue

package kube

import (
	"k8s.io/api/core/v1"
	apps_v1 "k8s.io/api/apps/v1"
)

// Mix in imported schema.
//
// Note that even though Service and Deployment are not defined as definitions,
// mixing in the generated definitions from k8s.io will make them behave
// as if they were.

Service:    v1.#Service
Deployment: apps_v1.#Deployment
```

These definitions in the `k8s` packages are the result of the import from Go
code. We plan to have a curated repository of those templates. But for now you
can generate them, as you will see below.

Note that with the change shown above, `cue cmd apply` no longer works:

<pre><code>$ cue cmd apply
import failed: cannot find package &#34;k8s.io/api/apps/v1&#34;:
    ../kube_defs.cue:5:2
</code></pre>

The `acme.com/x` CUE module does not know how to resolve the `k8s` packages. Fix
that by importing those definitions from go code. First step is to declare a Go
dependency on the relevant `k8s` packages:

```go
// /workdir/cue_deps.go

//go:build cue
// +build cue

package cue

import _ "k8s.io/api/apps/v1"
```

Then `go get` the module:

<pre><code>$ go get k8s.io/api/apps/v1@v0.23.2
</code></pre>

Now you can generate the CUE definitions from the Go types:

<pre><code>$ cue get go k8s.io/api/apps/v1
</code></pre>

`cue cmd apply` should now work again:

<pre><code>$ cue cmd apply
service/funquoter unchanged
deployment.apps/funquoter unchanged
</code></pre>

Not only that, but despite now also validating against the full imported schema
there is no change in the resulting configuration, i.e. no delta to apply.

You can verify that the constraints of the imported `k8s` definitions validate
typos in your configuration.  For example, try changing the `replicas` field in
`kube.yaml` to `replica` and then re-run `cue cmd apply`. Be sure to leave the
configuration in a working state!

### Going CUE native

At this point, the `funquoter` team are convinced. They want to go all in, and
maintain CUE files instead of YAML.

Certain language and syntax decisions in CUE make that an excellent choice
compared to either JSON or YAML especially because you can leave behind the
world of white space significance. But also optional braces, optional commas,
order irrelevance string interpolation, references, disjunctions, default
values, comprehensions, templates, packages, modules, etc.

The language tooling is also very powerful. `cue fmt` automatically formats CUE
code, and a work-in-progress [language server
protocol](https://github.com/cue-lang/cue/issues/142) implementation will bring
everything from code completion to validation in editor.

Not only that, as you will see `cue trim` allows you to automatically reduce
boilerplate in CUE configurations (something we also plan to support in JSON and
YAML).

Start by importing the existing YAML Kubernetes declarations to CUE using `cue
import`:

<pre><code>$ cue import -p kube -l &#39;objByKind:&#39; -l &#39;strings.ToCamel(kind)&#39; -l metadata.name kube.yaml
</code></pre>

The `-p` flag adds the resulting CUE to the `kube` package like all the other
CUE code The `-l` flag defines the path at which the imported YAML is placed. It
can appear multiple times Each imported `funquoter` object needs to be placed at
the path: `objByKind`, a field defined by the `kind` of object (for which you
can alter the casing using `strings.ToCamel`), and finally a field defined by
the `metadata.name` of the object.

Look at the result of directly importing YAML to CUE:

<pre><code>$ cat kube.cue
package kube

objByKind: service: funquoter: &#123;
	apiVersion: &#34;v1&#34;
	kind:       &#34;Service&#34;
	metadata: &#123;
		name: &#34;funquoter&#34;
		labels: app: &#34;funquoter&#34;
	&#125;
	spec: &#123;
		selector: app: &#34;funquoter&#34;
		ports: [&#123;
			protocol:   &#34;TCP&#34;
			port:       80
			targetPort: 3000
		&#125;]
	&#125;
&#125;
objByKind: deployment: funquoter: &#123;
	apiVersion: &#34;apps/v1&#34;
	kind:       &#34;Deployment&#34;
	metadata: name: &#34;funquoter&#34;
	spec: &#123;
		replicas: 1
		selector: matchLabels: app: &#34;funquoter&#34;
		template: &#123;
			metadata: labels: app: &#34;funquoter&#34;
			spec: containers: [&#123;
				name:            &#34;application&#34;
				image:           &#34;k3d-registry.acme.com:5000/fosdem2022/funquoter&#34;
				imagePullPolicy: &#34;Always&#34;
				args: [
					&#34;-addr=quoteserver:80&#34;,
					&#34;-requote=10s&#34;,
				]
			&#125;]
		&#125;
	&#125;
&#125;
</code></pre>

You can simplify the resulting CUE using `cue trim`, removing boiler plate that
is implied by other constraints and schemas:

<pre><code>$ cue trim -s
</code></pre>

Verify you can still `cue cmd apply`:

<pre><code>$ cue cmd apply
service/funquoter unchanged
deployment.apps/funquoter unchanged
</code></pre>

Significantly, there are no changes to be applied which validates that your
import from YAML to CUE, and boiler plate removal with `cue trim`, didn't add or
remove anything.

At this stage the YAML and CUE are living side by side, and both need to be
consistent. Make a change to the YAML at this point and re-run `apply`: you will
notice that `cmd/cue` complains of an inconsitency between the CUE constraints
and the YAML.  Given the `funquoter` team want to migrate away from YAML they
can simply remove this file and maintain the CUE file:

<pre><code>$ rm kube.yaml
$ cue cmd apply
service/funquoter unchanged
deployment.apps/funquoter unchanged
</code></pre>

At this point, the `funquoter` service has been fully converted from YAML to CUE.
The team can validate their configuration against the Kubernetes source of truth
definitions, and have removed unnecessary boilerplate that is implied by
constraints elsewhere.

In the process the resulting configuration has changed in only one respect from
the original configuration: namely the monitoring team's policy enforcing a
liveness probe.

